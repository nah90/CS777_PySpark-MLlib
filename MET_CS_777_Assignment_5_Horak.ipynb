{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MET CS 777 Assignment 5 Horak",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNufcvt5jNjD+pz9SPjBZIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nah90/CS777_PySpark-MLlib/blob/main/MET_CS_777_Assignment_5_Horak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --ignore-installed -q pyspark==3.2.1"
      ],
      "metadata": {
        "id": "cHSQE-2Q4-jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2CMo2Su44UY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys\n",
        "import numpy as np\n",
        "from operator import add\n",
        "from timeit import default_timer as timer\n",
        "import time\n",
        "\n",
        "\n",
        "from operator import add\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "JyX-L9s1-jHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5000"
      ],
      "metadata": {
        "id": "GkGL8qtO-kk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTrainingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile1 = 'SmallTrainingData.txt.bz2'\n",
        "corpus1 = sc.textFile(courtFile1, 1)"
      ],
      "metadata": {
        "id": "tqznWvZ9A0AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 1\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "task_1_timer = timer()\n",
        "\n",
        "keyAndTextTrain = corpus1.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTraindf = spark.createDataFrame(keyAndTextTrain).toDF(\"id\",\"label\",\"text\").cache()\n",
        "\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='words')\n",
        "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol='raw_features', vocabSize=vocab_size)\n",
        "idf = IDF(inputCol=cv.getOutputCol(), outputCol='features')\n",
        "\n",
        "pipeline1 = Pipeline(stages=[tokenizer, remover, cv, idf])\n",
        "model1 = pipeline1.fit(keyLabelTextTraindf)\n",
        "\n",
        "keyLabelTextTraindf.unpersist()\n",
        "\n",
        "print(\"Total time needed to vectorize data in seconds: \", timer() - task_1_timer)\n",
        "print(\"First 10 words from our vocabulary: 5k positions: \", model1.stages[-2].vocabulary[:10])"
      ],
      "metadata": {
        "id": "gYguLOvaCStb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29587113-f3f5-4070-d590-65f41c2b4dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to vectorize data in seconds:  24.39843286699761\n",
            "First 10 words from our vocabulary: 5k positions:  ['also', 'first', 'one', 'new', 'two', 'may', 'time', 'made', 'many', 'used']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTrainingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile1 = 'SmallTrainingData.txt.bz2'\n",
        "corpus1 = sc.textFile(courtFile1, 1)\n",
        "\n",
        "keyAndTextTrain = corpus1.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTraindf = spark.createDataFrame(keyAndTextTrain).toDF(\"id\",\"label\",\"text\").cache()\n",
        "\n",
        "task_2_timer = timer()\n",
        "training_2_timer = timer()\n",
        "\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='words')\n",
        "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol='raw_features', vocabSize=vocab_size)\n",
        "idf = IDF(inputCol=cv.getOutputCol(), outputCol='features')\n",
        "lr = LogisticRegression(maxIter=20, labelCol='label', featuresCol=idf.getOutputCol())\n",
        "\n",
        "pipeline2 = Pipeline(stages=[tokenizer, remover, cv, idf, lr])\n",
        "model2 = pipeline2.fit(keyLabelTextTraindf)\n",
        "\n",
        "keyLabelTextTraindf.unpersist()\n",
        "\n",
        "print(\"Total time needed to train model in seconds: \", timer() - training_2_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6SgNUa_TozY",
        "outputId": "19807448-5838-4eb0-8185-e0ef77d9e9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to train model in seconds:  30.204910868997104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTestingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile2 = 'SmallTestingData.txt.bz2'\n",
        "corpus2 = sc.textFile(courtFile2, 1)\n",
        "\n",
        "evaluation_2_timer = timer()\n",
        "\n",
        "keyAndTextTest = corpus2.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTestdf = spark.createDataFrame(keyAndTextTest).toDF(\"id\",\"label\",\"text\").cache()"
      ],
      "metadata": {
        "id": "Gq3nsPvLCtNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2 = model2.transform(keyLabelTextTestdf)\n",
        "\n",
        "keyLabelTextTestdf.unpersist()\n",
        "\n",
        "predictionAndLabels2 = predictions2.select(\"label\",\"prediction\").rdd.map(lambda x: (float(x[0]), float(x[1]))).cache()\n",
        "\n",
        "print(\"Total time needed to evaluate test data in seconds: \", timer() - evaluation_2_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s0w-OL3Ct3p",
        "outputId": "ede58c00-915d-41c9-e0f5-763cf8c7525f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to evaluate test data in seconds:  0.4945748509999248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_2_timer = timer()\n",
        "\n",
        "metrics2 = MulticlassMetrics(predictionAndLabels2)\n",
        "\n",
        "predictionAndLabels2.unpersist()\n",
        "\n",
        "print(\"F1 Score: \", metrics2.fMeasure(1.0))\n",
        "print(\"Confusion Matrix: \")\n",
        "print(metrics2.confusionMatrix().toArray().astype(int))\n",
        "print('')\n",
        "\n",
        "print(\"Total time needed to calculate performance metrics in seconds: \", timer() - metrics_2_timer)\n",
        "print(\"Total time needed to execute Task 2 in seconds: \", timer() - task_2_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTTo_QU8VlDS",
        "outputId": "47c31ce9-b6f3-46cc-ffac-9f170b89cfbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  1.0\n",
            "Confusion Matrix: \n",
            "[[20  0]\n",
            " [ 0 20]]\n",
            "\n",
            "Total time needed to calculate performance metrics in seconds:  0.5912671879996196\n",
            "Total time needed to execute Task 2 in seconds:  31.346847290998994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTrainingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile1 = 'SmallTrainingData.txt.bz2'\n",
        "corpus1 = sc.textFile(courtFile1, 1)\n",
        "\n",
        "keyAndTextTrain = corpus1.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTraindf = spark.createDataFrame(keyAndTextTrain).toDF(\"id\",\"label\",\"text\").cache()\n",
        "\n",
        "task_3_timer = timer()\n",
        "training_3_timer = timer()\n",
        "\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='words')\n",
        "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol='raw_features', vocabSize=vocab_size)\n",
        "idf = IDF(inputCol=cv.getOutputCol(), outputCol='features')\n",
        "svm = LinearSVC(maxIter=20, labelCol='label', featuresCol=idf.getOutputCol())\n",
        "\n",
        "pipeline3 = Pipeline(stages=[tokenizer, remover, cv, idf, svm])\n",
        "model3 = pipeline3.fit(keyLabelTextTraindf)\n",
        "\n",
        "keyLabelTextTraindf.unpersist()\n",
        "\n",
        "print(\"Total time needed to train model in seconds: \", timer() - training_3_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb_pRO-C8S6h",
        "outputId": "9424b463-a5ae-407a-e10e-5ae801e720a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to train model in seconds:  26.99593031599943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTestingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile2 = 'SmallTestingData.txt.bz2'\n",
        "corpus2 = sc.textFile(courtFile2, 1)\n",
        "\n",
        "evaluation_3_timer = timer()\n",
        "\n",
        "keyAndTextTest = corpus2.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTestdf = spark.createDataFrame(keyAndTextTest).toDF(\"id\",\"label\",\"text\").cache()"
      ],
      "metadata": {
        "id": "4W63RCdl8UPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions3 = model3.transform(keyLabelTextTestdf)\n",
        "\n",
        "keyLabelTextTestdf.unpersist()\n",
        "\n",
        "predictionAndLabels3 = predictions3.select(\"label\",\"prediction\").rdd.map(lambda x: (float(x[0]), float(x[1]))).cache()\n",
        "\n",
        "print(\"Total time needed to evaluate test data in seconds: \", timer() - evaluation_3_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe097dNl8UZl",
        "outputId": "b14a04b3-debc-4968-a553-5385b3a45483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to evaluate test data in seconds:  0.34237991900226916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_3_timer = timer()\n",
        "\n",
        "metrics3 = MulticlassMetrics(predictionAndLabels3)\n",
        "\n",
        "predictionAndLabels3.unpersist()\n",
        "\n",
        "print(\"F1 Score: \", metrics3.fMeasure(1.0))\n",
        "print(\"Confusion Matrix: \")\n",
        "print(metrics3.confusionMatrix().toArray().astype(int))\n",
        "print('')\n",
        "\n",
        "print(\"Total time needed to calculate performance metrics in seconds: \", timer() - metrics_3_timer)\n",
        "print(\"Total time needed to execute Task 3 in seconds: \", timer() - task_3_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL2iCEKa8Ugs",
        "outputId": "01bc4600-a266-42d1-9d96-26d77d181aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  1.0\n",
            "Confusion Matrix: \n",
            "[[20  0]\n",
            " [ 0 20]]\n",
            "\n",
            "Total time needed to calculate performance metrics in seconds:  0.500955994000833\n",
            "Total time needed to execute Task 3 in seconds:  27.91153830300027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector\n",
        "from pyspark.ml.classification import LogisticRegression, LinearSVC\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTrainingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile1 = 'SmallTrainingData.txt.bz2'\n",
        "corpus1 = sc.textFile(courtFile1, 1)\n",
        "\n",
        "keyAndTextTrain = corpus1.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTraindf = spark.createDataFrame(keyAndTextTrain).toDF(\"id\",\"label\",\"text\").cache()\n",
        "\n",
        "reduced_features = 200"
      ],
      "metadata": {
        "id": "ECq99gtpC_gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subtask A - Logistic Regression\n",
        "\n",
        "task_4a_timer = timer()\n",
        "training_4a_timer = timer()\n",
        "\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='words')\n",
        "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol='raw_features', vocabSize=vocab_size)\n",
        "idf = IDF(inputCol=cv.getOutputCol(), outputCol='features')\n",
        "selector = ChiSqSelector(numTopFeatures=reduced_features, labelCol='label', featuresCol=idf.getOutputCol(), outputCol='selected_features')\n",
        "lr = LogisticRegression(maxIter=20, labelCol='label', featuresCol=selector.getOutputCol())\n",
        "\n",
        "pipeline4a = Pipeline(stages=[tokenizer, remover, cv, idf, selector, lr])\n",
        "model4a = pipeline4a.fit(keyLabelTextTraindf)\n",
        "\n",
        "keyLabelTextTraindf.unpersist()\n",
        "\n",
        "print(\"Total time needed to train model in seconds: \", timer() - training_4a_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOk9LlESDHe-",
        "outputId": "8fd58511-fafa-4a70-df40-cdd9964ae664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to train model in seconds:  38.35479265699905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTestingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile2 = 'SmallTestingData.txt.bz2'\n",
        "corpus2 = sc.textFile(courtFile2, 1)\n",
        "\n",
        "evaluation_4a_timer = timer()\n",
        "\n",
        "keyAndTextTest = corpus2.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTestdf = spark.createDataFrame(keyAndTextTest).toDF(\"id\",\"label\",\"text\").cache()"
      ],
      "metadata": {
        "id": "mJZkKNMnEK_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions4a = model4a.transform(keyLabelTextTestdf)\n",
        "\n",
        "keyLabelTextTestdf.unpersist()\n",
        "\n",
        "predictionAndLabels4a = predictions4a.select(\"label\",\"prediction\").rdd.map(lambda x: (float(x[0]), float(x[1]))).cache()\n",
        "\n",
        "print(\"Total time needed to evaluate test data in seconds: \", timer() - evaluation_4a_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kj796bgELGi",
        "outputId": "426f5686-89f0-4789-9f00-b76b470aff44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to evaluate test data in seconds:  0.3559647359979863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_4a_timer = timer()\n",
        "\n",
        "metrics4a = MulticlassMetrics(predictionAndLabels4a)\n",
        "\n",
        "predictionAndLabels4a.unpersist()\n",
        "\n",
        "print(\"F1 Score: \", metrics4a.fMeasure(1.0))\n",
        "print(\"Confusion Matrix: \")\n",
        "print(metrics4a.confusionMatrix().toArray().astype(int))\n",
        "print('')\n",
        "\n",
        "print(\"Total time needed to calculate performance metrics in seconds: \", timer() - metrics_4a_timer)\n",
        "print(\"Total time needed to execute Task 4a in seconds: \", timer() - task_4a_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE5NM1_BELMs",
        "outputId": "bba7ce08-d146-4109-e58e-d8a1409baadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.888888888888889\n",
            "Confusion Matrix: \n",
            "[[20  4]\n",
            " [ 0 16]]\n",
            "\n",
            "Total time needed to calculate performance metrics in seconds:  0.5742057980023674\n",
            "Total time needed to execute Task 4a in seconds:  39.340379474000656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "courtFile1 = 'SmallTrainingData.txt.bz2'\n",
        "corpus3 = sc.textFile(courtFile1, 1)\n",
        "\n",
        "keyAndTextTrain = corpus3.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTraindf = spark.createDataFrame(keyAndTextTrain).toDF(\"id\",\"label\",\"text\").cache()"
      ],
      "metadata": {
        "id": "6UHZpR66TQPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Subtask B - Support Vector Machine\n",
        "\n",
        "task_4b_timer = timer()\n",
        "training_4b_timer = timer()\n",
        "\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='token_text')\n",
        "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='words')\n",
        "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol='raw_features', vocabSize=vocab_size)\n",
        "idf = IDF(inputCol=cv.getOutputCol(), outputCol='features')\n",
        "selector = ChiSqSelector(numTopFeatures=reduced_features, labelCol='label', featuresCol=idf.getOutputCol(), outputCol='selected_features')\n",
        "svm = LinearSVC(maxIter=20, labelCol='label', featuresCol=selector.getOutputCol())\n",
        "\n",
        "pipeline4b = Pipeline(stages=[tokenizer, remover, cv, idf, selector, svm])\n",
        "model4b = pipeline4b.fit(keyLabelTextTraindf)\n",
        "\n",
        "keyLabelTextTraindf.unpersist()\n",
        "\n",
        "print(\"Total time needed to train model in seconds: \", timer() - training_4b_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbdoZvCNEzxy",
        "outputId": "2c8ec47a-379e-48ea-bbd7-e8cd2fb2021a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to train model in seconds:  37.95938682600172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_4b_timer = timer()\n",
        "\n",
        "# ! wget -q https://storage.googleapis.com/met-cs-777-data/SmallTestingData.txt.bz2\n",
        "\n",
        "# Use this code to read the data\n",
        "courtFile2 = 'SmallTestingData.txt.bz2'\n",
        "corpus4 = sc.textFile(courtFile2, 1)\n",
        "\n",
        "evaluation_4a_timer = timer()\n",
        "\n",
        "keyAndTextTest = corpus4.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\\\n",
        "  .map(lambda x: (x[0], int(x[0].startswith(\"AU\")),x[1]))\n",
        "\n",
        "keyLabelTextTestdf = spark.createDataFrame(keyAndTextTest).toDF(\"id\",\"label\",\"text\").cache()"
      ],
      "metadata": {
        "id": "xHbcG8EkEz3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions4b = model4b.transform(keyLabelTextTestdf)\n",
        "\n",
        "keyLabelTextTestdf.unpersist()\n",
        "\n",
        "predictionAndLabels4b = predictions4b.select(\"label\",\"prediction\").rdd.map(lambda x: (float(x[0]), float(x[1]))).cache()\n",
        "\n",
        "print(\"Total time needed to evaluate test data in seconds: \", timer() - evaluation_4b_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VHgf74EEz98",
        "outputId": "335de8a5-dca4-4549-8a22-cf30df9a36e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time needed to evaluate test data in seconds:  0.3507673029998841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_4b_timer = timer()\n",
        "\n",
        "metrics4b = MulticlassMetrics(predictionAndLabels4b)\n",
        "\n",
        "predictionAndLabels4b.unpersist()\n",
        "\n",
        "print(\"F1 Score: \", metrics4b.fMeasure(1.0))\n",
        "print(\"Confusion Matrix: \")\n",
        "print(metrics4b.confusionMatrix().toArray().astype(int))\n",
        "print('')\n",
        "\n",
        "print(\"Total time needed to calculate performance metrics in seconds: \", timer() - metrics_4b_timer)\n",
        "print(\"Total time needed to execute Task 4a in seconds: \", timer() - task_4b_timer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pme3dFkTE0Er",
        "outputId": "c26f0f0b-ae82-47f5-ab72-6826cf6b8637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:  0.888888888888889\n",
            "Confusion Matrix: \n",
            "[[20  4]\n",
            " [ 0 16]]\n",
            "\n",
            "Total time needed to calculate performance metrics in seconds:  0.6152691170027538\n",
            "Total time needed to execute Task 4a in seconds:  38.96047091299988\n"
          ]
        }
      ]
    }
  ]
}